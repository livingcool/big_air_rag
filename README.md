# Big AIR Lab ‚Äî Multi-Modal Document Intelligence (RAG)

## üöÄ Project Overview

**Big AIR Lab** is a high-precision **Multi-Modal Retrieval-Augmented Generation (RAG)** system designed to answer complex quantitative questions from financial reports and technical PDF documents. Unlike standard RAG systems that treat tables as unstructured text streams, this engine utilizes **Vision Transformers (Table Transformer)** to structurally recognize tables as visual objects. It employs a custom coordinate-alignment algorithm to extract data with 100% fidelity and synthesizes answers using **Google's Gemini 1.5 Flash** for superior reasoning at zero cost.

### üß† Key Differentiators (The "Alpha")

* **Visual Structure Recognition:** Integrates Microsoft's *Table Transformer* to "see" table boundaries, headers, and rows, preventing the data soup problem common in PDF parsing.
* **Aggressive Data Alignment:** Implements a custom `padding & tolerance` algorithm to solve the "PDF-to-Image coordinate drift," ensuring numeric data is captured even when table borders are tight.
* **Asymmetric Cost Architecture:** A zero-cost operational model using **Local Embeddings (SentenceTransformers)** for vectorization and **Gemini 1.5 Flash** (Free Tier) for intelligence.
* **Decoupled & Scalable:** Split into a heavy computational **Backend (FastAPI + PyTorch)** and a lightweight **Frontend (Streamlit)**, allowing for optimized deployment on container-native platforms.

---

## üèóÔ∏è Architecture

The system is composed of two independent, communicating services:

### 1. RAG Processing Engine (Backend)

* **Framework:** FastAPI (Python 3.10+)
* **Core Logic:** Ingestion, OCR, Layout Analysis, Semantic Chunking, Vector Search.
* **ML Models:**
  * **Vision:** `microsoft/table-transformer-structure-recognition`
  * **Embeddings:** `all-MiniLM-L6-v2` (Hugging Face / SentenceTransformers)
* **Database:** Faiss (In-memory Vector Index for high-speed retrieval)
* **LLM:** Google Gemini 1.5 Flash (via Google GenAI API)

### 2. User Interface (Frontend)

* **Framework:** Streamlit
* **Function:** Document upload, Chat interface, Interactive evidence visualization (JSON/Text).

---

## üõ†Ô∏è Installation & Local Setup

### Prerequisites

* Python 3.10+
* Git
* Docker (Optional, recommended for stability)
* **Google Gemini API Key** (Get one free [here](https://aistudio.google.com/app/apikey))

### 1. Clone the Repository

```bash
git clone <repository-url>
cd big_air_rag
```

### 2. Environment Setup

Create a `.env` file in the root directory:

```bash
# .env
GEMINI_API_KEY=AIzaSy... # (Paste your actual key here)
API_BASE_URL=http://localhost:8000
```

### 3. Install Dependencies

It is recommended to use a virtual environment to manage the heavy ML libraries.

```bash
# Create Virtual Env
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Mac/Linux)
source venv/bin/activate

# Install requirements (Warning: Installs PyTorch, may take 5-10 mins)
pip install fastapi uvicorn streamlit python-dotenv pypdf pymupdf pillow pytesseract transformers torch torchvision timm faiss-cpu sentence-transformers google-generativeai
```

---

## üèÉ‚Äç‚ôÇÔ∏è Running the Application

You need two separate terminals to run the full stack locally.

### Terminal 1: The Backend (Brain)

This starts the API and loads the Table Transformer model into memory.

```bash
# Ensure you are in root directory and venv is active
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

Wait until you see:
```
INFO: Application startup complete.
```

### Terminal 2: The Frontend (Face)

This starts the web interface.

```bash
# Ensure you are in root directory and venv is active
streamlit run ui/streamlit_app.py
```

The app will open automatically in your browser at `http://localhost:8501`.

---

## üê≥ Docker Deployment (Recommended)

Since the backend relies on complex dependencies (PyTorch, Detectron2 components), Docker ensures consistency across environments.

### 1. Build the Image

```bash
docker build -t multimodal-rag-engine .
```

### 2. Run the Container

```bash
docker run -p 8000:8000 --env-file .env multimodal-rag-engine
```

---

## ‚òÅÔ∏è Deployment Strategy (The "Split Strategy")

Due to the memory requirements of the ML models, this project utilizes a split deployment architecture:

### Backend (Render):
* Hosts the Docker container with FastAPI and PyTorch.
* **Platform:** Render (Web Service / Docker Runtime).
* **Env Vars:** `GEMINI_API_KEY`, `PORT=8000`.

### Frontend (Streamlit Cloud):
* Hosts the lightweight UI code.
* **Platform:** Streamlit Community Cloud.
* **Secrets:** `API_BASE_URL` pointing to the Render backend URL.

---

## üìÇ Project Structure

```plaintext
big_air_rag/
‚îú‚îÄ‚îÄ app.py                      # FastAPI Entry Point (Orchestrates Ingestion & Query)
‚îú‚îÄ‚îÄ requirements.txt            # Production Dependencies (Frozen)
‚îú‚îÄ‚îÄ Dockerfile                  # Backend Container Configuration
‚îú‚îÄ‚îÄ .env                        # API Keys (Excluded from Git)
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py        # Frontend Client Code
‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îú‚îÄ‚îÄ pdf_processor.py        # PyMuPDF Logic + Coordinate Scaling Algorithm
‚îÇ   ‚îî‚îÄ‚îÄ table_extractor.py      # Hugging Face Table Transformer Logic
‚îú‚îÄ‚îÄ chunking/
‚îÇ   ‚îî‚îÄ‚îÄ chunker.py              # Semantic Text Splitter
‚îú‚îÄ‚îÄ embeddings/
‚îÇ   ‚îî‚îÄ‚îÄ embedder.py             # Local SentenceTransformers + Faiss Indexing
‚îú‚îÄ‚îÄ retrieval/
‚îÇ   ‚îî‚îÄ‚îÄ retriever.py            # Vector Search + MMR Reranking Logic
‚îî‚îÄ‚îÄ llm/
    ‚îî‚îÄ‚îÄ prompt_manager.py       # Google Gemini Integration & Prompt Engineering
```

---

## üß™ Performance & Benchmarks

* **Ingestion Speed:** ~2-3 seconds per page (limited by CPU inference of Table Transformer).
* **Extraction Precision:** >95% capture rate for numeric data in dense financial tables.
* **Reasoning Fidelity:** Successfully answers multi-hop questions (e.g., "Compare 2022 GDP vs 2025 Hydrocarbon Growth") by citing specific table cells.

---

**Author:** Ganesh K
**License:** MIT